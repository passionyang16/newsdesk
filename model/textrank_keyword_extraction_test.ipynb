{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import kss\n",
    "import re\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mecab 객체 생성\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df 불러오기\n",
    "df = pd.read_csv(\"./test.csv\", header=None)\n",
    "df.columns = ['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거\n",
    "df = df.drop_duplicates('article')\n",
    "df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords list화\n",
    "temp = pd.read_csv(\"./stop_words.csv\",header = None, usecols = [0], names = ['stopwords'])                                                \n",
    "stopwords = []\n",
    "for i in range(temp.shape[0]):\n",
    "    stopwords.append(temp['stopwords'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(df)):\n",
    "    removed = re.sub('[-=+,#/\\?©:^$©@*\\\"※~&.·%｜!』\\\\‘◇”|\\(\\)\\[\\]\\<\\>`\\'’“…》]', '', df['article'][i])\n",
    "    tokenized = mecab.morphs(removed)\n",
    "    for j in range(len(tokenized)-1,-1,-1):\n",
    "        if tokenized[j] in stopwords:\n",
    "            tokenized.remove(tokenized[j])\n",
    "    result.append(tokenized)\n",
    "df['token'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['영국',\n",
       " '옥스퍼드',\n",
       " '대',\n",
       " '신종',\n",
       " '코로나',\n",
       " '바이러스',\n",
       " '감염증',\n",
       " '코로나',\n",
       " '19',\n",
       " '백신',\n",
       " '연구',\n",
       " '로이터',\n",
       " '뉴스',\n",
       " '1',\n",
       " '서울',\n",
       " '뉴스',\n",
       " '1',\n",
       " '한상희',\n",
       " '기자',\n",
       " '전',\n",
       " '세계',\n",
       " '신종',\n",
       " '코로나',\n",
       " '바이러스',\n",
       " '감염증',\n",
       " '코로나',\n",
       " '19',\n",
       " '백신',\n",
       " '연구',\n",
       " '중',\n",
       " '중',\n",
       " '가장',\n",
       " '앞선',\n",
       " '평가',\n",
       " '받',\n",
       " '는',\n",
       " '영국',\n",
       " '옥스퍼드',\n",
       " '대',\n",
       " '연구',\n",
       " '진이',\n",
       " '최종',\n",
       " '임상',\n",
       " '3',\n",
       " '상',\n",
       " '시험',\n",
       " '긍정',\n",
       " '적',\n",
       " '인',\n",
       " '결과',\n",
       " '얻',\n",
       " '었',\n",
       " '다고',\n",
       " '1',\n",
       " '현지',\n",
       " '밝혔',\n",
       " '다',\n",
       " '로이터',\n",
       " '통신',\n",
       " '따르',\n",
       " '면',\n",
       " '사라',\n",
       " '길버트',\n",
       " '옥스퍼드',\n",
       " '대',\n",
       " '의대',\n",
       " '교수',\n",
       " '는',\n",
       " '이날',\n",
       " '의회',\n",
       " '출석',\n",
       " '코로나',\n",
       " '19',\n",
       " '백신',\n",
       " '임상',\n",
       " '3',\n",
       " '상',\n",
       " '진행',\n",
       " '상황',\n",
       " '대한',\n",
       " '질문',\n",
       " '백신',\n",
       " '후보',\n",
       " '물질',\n",
       " '적절',\n",
       " '한',\n",
       " '종류',\n",
       " '면역',\n",
       " '반응',\n",
       " '보였',\n",
       " '다',\n",
       " '고',\n",
       " '답',\n",
       " '했',\n",
       " '다',\n",
       " '옥스퍼드',\n",
       " '대',\n",
       " '는',\n",
       " '영국',\n",
       " '제약사',\n",
       " '아스트라',\n",
       " '카',\n",
       " '인체',\n",
       " '해',\n",
       " '없',\n",
       " '는',\n",
       " '바이러스',\n",
       " '코로나',\n",
       " '바이러스',\n",
       " '유전자',\n",
       " '끼워',\n",
       " '넣',\n",
       " '은',\n",
       " '유전자',\n",
       " '재조합',\n",
       " '백신',\n",
       " 'AZD',\n",
       " '1222',\n",
       " '개발',\n",
       " '이달',\n",
       " '8000',\n",
       " '명',\n",
       " '임상시험',\n",
       " '자원자',\n",
       " '대상',\n",
       " '3',\n",
       " '상',\n",
       " '시험',\n",
       " '고',\n",
       " '있',\n",
       " '다',\n",
       " '앞서',\n",
       " '세계',\n",
       " '보건',\n",
       " '기구',\n",
       " 'WHO',\n",
       " '는',\n",
       " '세계',\n",
       " '백신',\n",
       " '연구',\n",
       " '중',\n",
       " '옥스퍼드',\n",
       " '아스트라',\n",
       " '제네',\n",
       " '카가',\n",
       " '가장',\n",
       " '앞서',\n",
       " '있',\n",
       " '다',\n",
       " '고',\n",
       " '평가',\n",
       " '기',\n",
       " '도',\n",
       " '했',\n",
       " '다',\n",
       " '길버트',\n",
       " '교수',\n",
       " '는',\n",
       " '9',\n",
       " '말',\n",
       " '10',\n",
       " '백신',\n",
       " '100',\n",
       " '만',\n",
       " '회분',\n",
       " '생산',\n",
       " '계획',\n",
       " '는',\n",
       " '백신',\n",
       " '준비',\n",
       " '될',\n",
       " '지',\n",
       " '는',\n",
       " '시험',\n",
       " '결과',\n",
       " '달려',\n",
       " '있',\n",
       " '다',\n",
       " '며',\n",
       " '지금',\n",
       " '시점',\n",
       " '백신',\n",
       " '개발',\n",
       " '시간표',\n",
       " '제시',\n",
       " '할',\n",
       " '순',\n",
       " '없',\n",
       " '다',\n",
       " '고',\n",
       " '신중',\n",
       " '한',\n",
       " '태도',\n",
       " '보였',\n",
       " '다']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 모델링\n",
    "# news_tokens = df['tokenized'].tolist()  # data input as list\n",
    "\n",
    "# params\n",
    "v_dimension = 300\n",
    "v_window = 8\n",
    "model = Word2Vec.load('word2vec.model')\n",
    "\n",
    "# model = Word2Vec(sentences = news_tokens, size = v_dimension, window = v_window, min_count = 5, workers = 4, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#벡터화 작업\n",
    "def vectorize(tokens):\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        vectors = []\n",
    "        for sentence in token:\n",
    "            init_v = np.array([0.0]*v_dimension)\n",
    "            count = 0\n",
    "            for word in sentence:\n",
    "                word_vectors = model.wv\n",
    "                if word in word_vectors.vocab:\n",
    "                    count +=1\n",
    "                    v = model.wv[word]\n",
    "                    init_v = init_v + v\n",
    "            init_v = init_v / count    \n",
    "            vectors.append(init_v)\n",
    "        result.append(vectors)\n",
    "    \n",
    "    frame = {'article' : df['article'], 'token': df['token'], 'vector': result }\n",
    "    result = pd.DataFrame(frame) \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vectorize(df['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코사인유사도 매트릭스\n",
    "def similarity_matrix(vectors):\n",
    "    sim_mat = np.zeros([len(vectors), len(vectors)])\n",
    "    for i in tqdm(range(len(vectors))):\n",
    "        for j in range(len(vectors)):\n",
    "            sim_mat[i][j] = cosine_similarity(vectors[i].reshape(1, v_dimension), vectors[j].reshape(1, v_dimension))[0,0]\n",
    "    return sim_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:12<00:00, 16.01it/s]\n",
      "100%|██████████| 228/228 [00:16<00:00, 13.94it/s]\n",
      "100%|██████████| 183/183 [00:10<00:00, 17.50it/s]\n",
      "100%|██████████| 70/70 [00:01<00:00, 43.75it/s]\n",
      "100%|██████████| 129/129 [00:05<00:00, 24.34it/s]\n"
     ]
    }
   ],
   "source": [
    "df['SimMatrix'] = df['vector'].apply(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>token</th>\n",
       "      <th>vector</th>\n",
       "      <th>SimMatrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>영국 옥스퍼드대 신종 코로나바이러스 감염증 코로나19 백신 연구. © 로이터 뉴스1...</td>\n",
       "      <td>[영국, 옥스퍼드, 대, 신종, 코로나, 바이러스, 감염증, 코로나, 19, 백신,...</td>\n",
       "      <td>[[0.06985935149714351, -0.025657336227595806, ...</td>\n",
       "      <td>[[0.9999999999999998, -0.1314286774940165, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>크리스탈지노믹스가 수술이 힘든 진행성췌장암 환자에게 아이발티노스타트와 젬시타빈 얼로...</td>\n",
       "      <td>[크리스탈, 지노, 믹스, 수술, 힘든, 진행, 성, 췌장암, 환자, 발티노스, 타...</td>\n",
       "      <td>[[-0.08433880284428596, -0.8906333353370428, -...</td>\n",
       "      <td>[[0.9999999999999999, -0.09545740665889839, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>제주시는 최근 “신산공원 내 저류지를 개방해 반려동물 자율쉼터를 운영겠다”고 했다....</td>\n",
       "      <td>[제주, 시, 는, 최근, 신산, 공원, 내, 저류지, 개방, 해, 반려, 동물, ...</td>\n",
       "      <td>[[0.27231060713529587, 0.7067987825721502, 0.6...</td>\n",
       "      <td>[[0.9999999999999999, 0.4185591005579885, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5월엔 43.1 서울경제 미국 공급관리협회 ISM 는 1일 현지시간 6월 제조업 구...</td>\n",
       "      <td>[5, 엔, 431, 서울경제, 미국, 공급, 관리, 협회, ISM, 는, 1, 현...</td>\n",
       "      <td>[[1.2809516191482544, -0.4342169165611267, 2.3...</td>\n",
       "      <td>[[1.0000000000000004, 0.12217151691247938, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>서울경제 극자외선용 EUV 포토레지스트를 생산하는 일본 기업 TOK가 국내에서 생산...</td>\n",
       "      <td>[서울경제, 극자외선, 용, EUV, 포토레지스트, 생산, 는, 일본, 기업, TO...</td>\n",
       "      <td>[[0.06795791210606694, -0.020151667529717088, ...</td>\n",
       "      <td>[[0.9999999999999999, 0.3334628596227865, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  영국 옥스퍼드대 신종 코로나바이러스 감염증 코로나19 백신 연구. © 로이터 뉴스1...   \n",
       "1  크리스탈지노믹스가 수술이 힘든 진행성췌장암 환자에게 아이발티노스타트와 젬시타빈 얼로...   \n",
       "2  제주시는 최근 “신산공원 내 저류지를 개방해 반려동물 자율쉼터를 운영겠다”고 했다....   \n",
       "3  5월엔 43.1 서울경제 미국 공급관리협회 ISM 는 1일 현지시간 6월 제조업 구...   \n",
       "4  서울경제 극자외선용 EUV 포토레지스트를 생산하는 일본 기업 TOK가 국내에서 생산...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [영국, 옥스퍼드, 대, 신종, 코로나, 바이러스, 감염증, 코로나, 19, 백신,...   \n",
       "1  [크리스탈, 지노, 믹스, 수술, 힘든, 진행, 성, 췌장암, 환자, 발티노스, 타...   \n",
       "2  [제주, 시, 는, 최근, 신산, 공원, 내, 저류지, 개방, 해, 반려, 동물, ...   \n",
       "3  [5, 엔, 431, 서울경제, 미국, 공급, 관리, 협회, ISM, 는, 1, 현...   \n",
       "4  [서울경제, 극자외선, 용, EUV, 포토레지스트, 생산, 는, 일본, 기업, TO...   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [[0.06985935149714351, -0.025657336227595806, ...   \n",
       "1  [[-0.08433880284428596, -0.8906333353370428, -...   \n",
       "2  [[0.27231060713529587, 0.7067987825721502, 0.6...   \n",
       "3  [[1.2809516191482544, -0.4342169165611267, 2.3...   \n",
       "4  [[0.06795791210606694, -0.020151667529717088, ...   \n",
       "\n",
       "                                           SimMatrix  \n",
       "0  [[0.9999999999999998, -0.1314286774940165, 0.2...  \n",
       "1  [[0.9999999999999999, -0.09545740665889839, 0....  \n",
       "2  [[0.9999999999999999, 0.4185591005579885, -0.0...  \n",
       "3  [[1.0000000000000004, 0.12217151691247938, 0.7...  \n",
       "4  [[0.9999999999999999, 0.3334628596227865, -0.0...  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코사인 유사도를 활용해서 각 문장마다 점수 매기기\n",
    "#pagerank 사용\n",
    "def calculate_score(sim_matrix):\n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------- 이거 이렇게 하면 안될듯... word 하나당 벡터 300개씩 해버리면 계산량이 개지옥임..... 아무래도 keyword extraction은 tf-idf 써야할듯 ㅠㅠ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: https://lovit.github.io/nlp/2019/04/30/textrank/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
